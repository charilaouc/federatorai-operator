apiVersion: v1
kind: ConfigMap
metadata:
  name: federatorai-data-adapter-config
  namespace: {{.NameSpace}}
data:
    telegraf.conf: |+
      [global_tags]

      [agent]
        interval = "1m"
        round_interval = true
        metric_batch_size = 1000
        metric_buffer_limit = 10000
        collection_jitter = "0s"
        flush_interval = "30s"
        flush_jitter = "0s"
        precision = "1us"
        debug = true
        logfile = "/var/log/telegraf.log"
        logfile_rotation_interval = "1d"
        logfile_rotation_max_archives = 7
        logfile_rotation_max_size = "100MB"
        ## Query will retry if some metrics hasn't returned yet.
        max_retry = 2
        retry_interval = "15s"
        quiet = false
        hostname = ""
        omit_hostname = false

      [[aggregators.basicstats]]
        period = "1m"
        granularity = "1m"
        drop_original = true
        namedrop = ["federatorai.*"]

      #[[aggregators.federatorai]]
      #  period = "1m"
      #  drop_original = true
      #  namepass = ["kafka_topic_partition_current_offset","kafka_consumer_group_current_offset","kafka_consumer_group_lag"]

      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_topic_partition_current_offset"
      #      expression = "sum(delta(kafka_topic_partition_current_offset[1m]))by(consumer_group,topic,namespace)"
      #      fields = ["value"]
      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_consumer_group_current_offset"
      #      expression = "sum(delta(kafka_consumer_group_current_offset[1m]))by(consumer_group,topic,namespace)"
      #      fields = ["value"]
      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_consumer_group_lag"
      #      expression = "sum(kafka_consumer_group_lag)by(namespace,consumer_group,topic)"
      #      fields = ["value"]

      [[inputs.alameda_datahub_query]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"
        ##The recommendation query range, unit: minutes
        recommendation_interval = 5
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          measurement = "node"
          scope = "prediction"


      [[outputs.datahub]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"

      #[[inputs.prometheus_query]]
      #  url = "$PROMETHEUS_URL"
      #  token_path = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      #  insecure_skip_verify = true
      #  watched_source = [{name="kafka_topic_partition_current_offset",expr="sum(rate(kafka_topic_partition_current_offset[1m]))by(consumergroup,topic,namespace)",tags=["consumergroup","topic","namespace"]},{name="kafka_consumer_group_current_offset",expr="sum(rate(kafka_consumergroup_current_offset[1m]))by(consumergroup,topic,namespace)",tags=["consumergroup","topic","namespace"]},{name="kafka_consumer_group_lag",expr="sum(kafka_consumergroup_lag)by(namespace,consumergroup,topic)",tags=["consumergroup","topic","namespace"]}]

      [[outputs.datadog]]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        User-Agent = "Federator.ai/4.2"
        kafka_dashboards = ["/etc/telegraf/dashboards/datadog/kafka/overview.json"]
        general_dashboards = ["/etc/telegraf/dashboards/datadog/kubernetes/application-overview.json"]
        enable_kafka_dashboard = false
        enable_general_dashboard = false

        # This pattern support federatorai.integration.status, federatorai.recommendation and federator.predictoin.*
        [[outputs.datadog.integration_metrics]]
          name="federatorai.*"
          aggregation_type="raw"

        [[outputs.datadog.integration_metrics]]
          name="kafka_topic_partition_current_offset"
          aggregation_type="diff"

        [[outputs.datadog.integration_metrics]]
          name="kafka_consumer_group_current_offset"
          aggregation_type="diff"

      # anchor
      [[inputs.datadog]]
        urls = ["$DATADOG_QUERY_URL"]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        ## If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"
        ## Cloud metric has 5 minutes to 10 minutes delay
        cloud_metric_delay_interval = "30m0s"
        ## Set default cloud information ifneeded
        enable_set_default_cloud_info_if_empty = true
        default_provider = "aws"
        default_region = "us-west-1"
        default_instance_type = "m5.4xlarge"
        default_instance_id = "i-00cd730e045190cad"
        default_zone = "us-west-1a"
        excluded_namespaces = ["kube-public", "kube-service-catalog", "kube-system", "management-infra", "kube-node-lease", "stackpoint-system", "marketplace", "openshift", "openshift-*"]
        # Watched source
        # TOML format reference: https://github.com/influxdata/toml/blob/master/README.md
        [[inputs.datadog.watched_source]]
          namespace = "<monitored_application_namespace>"
          application = "<monitored_application>"
          min_replicas = 0 # monitored_application_min_replicas
          max_replicas = 0 # monitored_application_max_replicas
          [[inputs.datadog.watched_source.watched_metrics]]
            name="kubernetes.cpu.usage.total"
            metric_type="CPU_MILLICORES_USAGE"
          [[inputs.datadog.watched_source.watched_metrics]]
            name="kubernetes.memory.usage"
            metric_type="MEMORY_BYTES_USAGE"

      [[inputs.alameda_datahub_query]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"
        ##The recommendation query range, unit: minutes
        recommendation_interval = 5
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "prediction"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "recommendation"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "planning"

      [[inputs.datadog_application_aware]]
        urls = ["$DATADOG_QUERY_URL"]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [inputs.datadog_application_aware.watched_kafka_consumer]
          application = ""
          namespace = ""
          min_replicas = 0
          max_replicas = 0
          topics = ["<kafka_topic_name>"]
          consumer_groups = ["<kafka_consumer_group_name>"]

      [[inputs.alameda_datahub_query]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"
        ##The recommendation query range, unit: minutes
        recommendation_interval = 5
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_consumer_group_name>"
          namespace = "<kafka_consumer_group_namespace>"
          measurement = "kafka_consumer_group"
          scope = "recommendation"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_consumer_group_name>"
          namespace = "<kafka_consumer_group_namespace>"
          measurement = "kafka_consumer_group_current_offset"
          scope = "prediction"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_topic_name>"
          namespace = "<kafka_topic_namespace>"
          measurement = "kafka_topic_partition_current_offset"
          scope = "prediction"
