apiVersion: v1
kind: ConfigMap
metadata:
  name: federatorai-data-adapter-config
  namespace: {{.NameSpace}}
data:
    telegraf.conf: |+
      [global_tags]

      [agent]
        interval = "1m"
        round_interval = true
        metric_batch_size = 1000
        metric_buffer_limit = 10000
        collection_jitter = "0s"
        flush_interval = "30s"
        flush_jitter = "0s"
        precision = "1us"
        debug = true
        logfile = "/var/log/telegraf.log"
        logfile_rotation_interval = "1d"
        logfile_rotation_max_archives = 7
        logfile_rotation_max_size = "100MB"
        quiet = false
        hostname = ""
        omit_hostname = false

      [[aggregators.basicstats]]
        period = "1m"
        granularity = "1m"
        drop_original = true
        namedrop = ["federatorai.*"]

      #[[aggregators.federatorai]]
      #  period = "1m"
      #  drop_original = true
      #  namepass = ["kafka_topic_partition_current_offset","kafka_consumer_group_current_offset","kafka_consumer_group_lag"]

      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_topic_partition_current_offset"
      #      expression = "sum(delta(kafka_topic_partition_current_offset[1m]))by(consumer_group,topic,namespace)"
      #      fields = ["value"]
      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_consumer_group_current_offset"
      #      expression = "sum(delta(kafka_consumer_group_current_offset[1m]))by(consumer_group,topic,namespace)"
      #      fields = ["value"]
      #  [[aggregators.federatorai.metric]]
      #      measurement_name = "kafka_consumer_group_lag"
      #      expression = "sum(kafka_consumer_group_lag)by(namespace,consumer_group,topic)"
      #      fields = ["value"]

      [[outputs.datahub]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"

      #[[inputs.prometheus_query]]
      #  url = "$PROMETHEUS_URL"
      #  token_path = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      #  insecure_skip_verify = true
      #  watched_source = [{name="kafka_topic_partition_current_offset",expr="sum(rate(kafka_topic_partition_current_offset[1m]))by(consumergroup,topic,namespace)",tags=["consumergroup","topic","namespace"]},{name="kafka_consumer_group_current_offset",expr="sum(rate(kafka_consumergroup_current_offset[1m]))by(consumergroup,topic,namespace)",tags=["consumergroup","topic","namespace"]},{name="kafka_consumer_group_lag",expr="sum(kafka_consumergroup_lag)by(namespace,consumergroup,topic)",tags=["consumergroup","topic","namespace"]}]

      [[outputs.datadog]]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        User-Agent = "Federator.ai/4.2"
        kafka_dashboards = ["/etc/telegraf/dashboards/datadog/kafka/overview.json"]
        general_dashboards = ["/etc/telegraf/dashboards/datadog/kubernetes/application-overview.json", "/etc/telegraf/dashboards/datadog/kubernetes/insight-overview.json"]
        enable_kafka_dashboard = false
        enable_general_dashboard = false

        # This pattern support federatorai.integration.status, federatorai.recommendation and federator.predictoin.*
        [[outputs.datadog.integration_metrics]]
          name="federatorai.*"
          aggregation_type="raw"

        [[outputs.datadog.integration_metrics]]
          name="kafka_topic_partition_current_offset"
          aggregation_type="diff"

        [[outputs.datadog.integration_metrics]]
          name="kafka_consumer_group_current_offset"
          aggregation_type="diff"

      # anchor
      [[inputs.datadog]]
        urls = ["$DATADOG_QUERY_URL"]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"
        # Watched source
        # TOML format reference: https://github.com/influxdata/toml/blob/master/README.md
        [[inputs.datadog.watched_source]]
          namespace = "<monitored_application_namespace>"
          application = "<monitored_application>"
          [[inputs.datadog.watched_source.watched_metrics]]
            name="kubernetes.cpu.usage.total"
            metric_type="CPU_MILLICORES_USAGE"
          [[inputs.datadog.watched_source.watched_metrics]]
            name="kubernetes.memory.usage"
            metric_type="MEMORY_BYTES_USAGE"

      [[inputs.alameda_datahub_query]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"
        ##The recommendation query range, unit: minutes
        recommendation_interval = 5
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "prediction"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "recommendation"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "planning"

      [[inputs.datadog_application_aware]]
        urls = ["$DATADOG_QUERY_URL"]
        api_key = "$DATADOG_API_KEY"
        application_key = "$DATADOG_APPLICATION_KEY"
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [inputs.datadog_application_aware.watched_kafka_consumer]
          application = ""
          namespace = ""
          min_replicas = 0
          max_replicas = 0
          topics = ["<kafka_topic_name>"]
          consumer_groups = ["<kafka_consumer_group_name>"]

      [[inputs.alameda_datahub_query]]
        url = "$DATAHUB_URL"
        port = "$DATAHUB_PORT"
        ##The recommendation query range, unit: minutes
        recommendation_interval = 5
        # If we keep CLUSTER_NAME value empty, the agent will get k8s cluster name automatically.
        cluster_name = "$CLUSTER_NAME"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_consumer_group_name>"
          namespace = "<kafka_consumer_group_namespace>"
          measurement = "kafka_consumer_group"
          scope = "recommendation"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_consumer_group_name>"
          namespace = "<kafka_consumer_grou_namespace>"
          measurement = "kafka_consumer_group_current_offset"
          scope = "prediction"

        [[inputs.alameda_datahub_query.watched_source]]
          name = "<kafka_topic_name>"
          namespace = "<kafka_topic_namespace>"
          measurement = "kafka_topic_partition_current_offset"
          scope = "prediction"

        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "prediction"
        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "recommendation"
        [[inputs.alameda_datahub_query.watched_source]] #General Application config
          name = "<general_application_name>"
          namespace = "<general_application_namespace>"
          measurement = "controller"
          scope = "planning"
