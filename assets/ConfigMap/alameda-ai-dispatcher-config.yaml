apiVersion: v1
kind: ConfigMap
metadata:
  name: alameda-ai-dispatcher-config
  namespace: {{.NameSpace}}
data:
  ai-dispatcher.toml: |-
    hourlyPredict = false

    [watchdog]
    delayedSec = 120
      [watchdog.model]
      directory = "/tmp/model"
      [watchdog.predict]
      directory = "/tmp/predict"

    [datahub]
    address = "datahub.alameda.svc.cluster.local:50050"
    connRetry = 5
      [datahub.query]
      retry = 3
      retryInterval = 10 # seconds
    
    [queue]
    url = "amqp://admin:adminpass@rabbitmq.alameda.svc.cluster.local:5672"
      [queue.retry]
      publishTime = 5
      publishIntervalMs = 3000
      consumeTime = 5
      consumeIntervalMs = 3000
      connectIntervalMs = 3000
      ackTimeoutSec = 3
      [queue.consumer]
      reconnectInterval = 30 #seconds
    
    [serviceSetting]
    granularities = ["30s", "1m", "1h", "6h", "24h"]
    predictUnits = ["POD", "GPU", "NAMESPACE",
      "APPLICATION", "CLUSTER", "CONTROLLER", "NODE"
    ]
    # must put NODE predict unit at last, because to send
    # NODE jobs with granularity 30s depends on POD job
    # with granularity 30s are sent
    
    [granularities]
    
      [granularities.24h]
      dataGranularity = "24h"
      dataGranularitySec = 86400
      predictionSteps = 30
      predictionJobSendIntervalSec = 86400
      modelJobSendIntervalSec = 86400
    
      [granularities.6h]
      dataGranularity = "6h"
      dataGranularitySec = 21600
      predictionSteps = 30
      predictionJobSendIntervalSec = 21600
      modelJobSendIntervalSec = 21600
    
      [granularities.1h]
      dataGranularity = "1h"
      dataGranularitySec = 3600
      predictionSteps = 30
      predictionJobSendIntervalSec = 3600
      modelJobSendIntervalSec = 3600
    
      [granularities.1m]
      dataGranularity = "1m"
      dataGranularitySec = 60
      predictionSteps = 60
      predictionJobSendIntervalSec = 60
      modelJobSendIntervalSec = 60
    
      [granularities.30s]
      dataGranularity = "30s"
      dataGranularitySec = 30
      predictionSteps = 30
      predictionJobSendIntervalSec = 30
      modelJobSendIntervalSec = 30
    
    [predictUnits]
    
      [predictUnits.POD]
      type = "POD"
    
      [predictUnits.NODE]
      type = "NODE"
    
      [predictUnits.GPU]
      type = "GPU"
    
      [predictUnits.NAMESPACE]
      type = "NAMESPACE"
    
      [predictUnits.APPLICATION]
      type = "APPLICATION"
    
      [predictUnits.CLUSTER]
      type = "CLUSTER"
    
      [predictUnits.CONTROLLER]
      type = "CONTROLLER"
    
    [log]
    setLogcallers = true
    outputLevel = "info" # debug, info, warn, error, fatal, none
    
    [model]
    enabled = false
    timeout = 180
    
    [measurements]
      current = "mape"
      minimumDataPoints = 5
      maximumDataPoints = 5
      [measurements.mape]
      threshold = 15
      [measurements.rmse]
      threshold = 10
        [measurements.rmse.normalization]
        cpu = 1 #millicores
        memory = 1000000 #bytes
        dutyCycle = 0.2
    
    # api proto metric type
    [metricType]
    undefined = 0
    cpu_seconds_total = 1
    cpu_millicores_total = 2
    cpu_millicores_avail = 3
    cpu_millicores_usage = 4
    cpu_millicores_usage_pct = 5
    cpu_millicores_allocatable = 6
    memory_bytes_total = 7
    memory_bytes_avail = 8
    memory_bytes_usage = 9
    memory_bytes_usage_pct = 10
    memory_bytes_allocatable = 11
    fs_bytes_total = 12
    fs_bytes_avail = 13
    fs_bytes_usage = 14
    fs_bytes_usage_pct = 15
    http_requests_count = 16
    http_requests_total = 17
    http_response_count = 18
    http_response_total = 19
    disk_io_seconds_total = 20
    disk_io_utilization = 21
    restarts_total = 22
    power_usage_watts = 23
    temperature_celsius = 24
    duty_cycle = 25
    current_offset = 26
    lag = 27
    latency = 28
    number = 29
    
    # api proto table
    [scope]
    undefined = 0
    application = 1
    metric = 2
    planning = 3
    prediction = 4
    recommendation = 5
    resource = 6
    
    # api proto aggregation function
    [aggregation]
    none = 0
    max = 1
    avg = 2
    
    [[units]]
    enabled = true
    scope = "application"
    category = "kafka"
    type = "topic"
    measurement = "kafka_topic"
    idKeys = ["cluster_name", "namespace", "name"]
    granularities = ["1m"]
    metricTypes = ["current_offset"]
    predictor = "SARIMAX"
    [units.valueKeys]
      scalerNamespace = "alameda_scaler_namespace"
      scalerName = "alameda_scaler_name"
    
      [units.metric]
      scope = "metric"
      category = "kafka"
      type = "topic"
      aggregation = "avg"
        [units.metric.valueKeys]
        value = "value"
    
      [units.prediction]
      scope = "prediction"
      category = "kafka"
      type = "topic"
        [units.prediction.valueKeys]
        modelID = "model_id"
        predictID = "prediction_id"
        granularity = "granularity"
        value = "value"
    
    
    [[units]]
    enabled = true
    scope = "application"
    category = "kafka"
    type = "consumer_group"
    measurement = "kafka_consumer_group"
    idKeys = ["cluster_name", "namespace", "name", "topic_name"]
    granularities = ["1m"]
    metricTypes = ["current_offset"]
    predictor = "SARIMAX"
    [units.valueKeys]
      scalerNamespace = "alameda_scaler_namespace"
      scalerName = "alameda_scaler_name"
      resourceK8SNamespace = "resource_k8s_namespace"
      resourceK8SName = "resource_k8s_name"

      [units.metric]
      scope = "metric"
      category = "kafka"
      type = "consumer_group"
      aggregation = "avg"
        [units.metric.valueKeys]
        value = "value"
    
      [units.prediction]
      scope = "prediction"
      category = "kafka"
      type = "consumer_group"
        [units.prediction.valueKeys]
        modelID = "model_id"
        predictID = "prediction_id"
        granularity = "granularity"
        value = "value"