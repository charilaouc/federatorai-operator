apiVersion: v1
kind: ConfigMap
metadata:
  name: federatorai-agent-app-config
  namespace: {{.NameSpace}}
data:
  transmitter.toml: |-
    [log]
     set-logcallers = true
     output-level = "info" # debug, info, warn, error, fatal, none

    [input_jobs]
    {{if .Kafka.Enabled}}
        [input_jobs.kafka]
        name = "kafka"
        schedule-spec = "@every 30s"
        lib-path = "/lib/inputlib/inputlib_app.so"
        lib-configuration = "/etc/alameda/federatorai-agent/input/alameda_kafka.toml"
    {{end}}

    {{if .Nginx.Enabled}}
        [input_jobs.nginx]
        name = "nginx"
        schedule-spec = "@every 15s"
        lib-path = "/lib/inputlib/inputlib_app.so"
        lib-configuration = "/etc/alameda/federatorai-agent/input/alameda_app.toml"

        [input_jobs.kubernetes]
        name = "kubernetes"
        schedule-spec = "@every 60s"
        lib-path = "/lib/inputlib/inputlib_kubernetes.so"
        lib-configuration = "/etc/alameda/federatorai-agent/input/kubernetes.toml"
    {{end}}

    {{if .ClusterAutoScaler.EnableExecution}}
        [input_jobs.node]
        name = "node"
        schedule-spec = "@every 60s"
        lib-path = "/lib/inputlib/inputlib_app.so"
        lib-configuration = "/etc/alameda/federatorai-agent/input/alameda_node.toml"
    {{end}}

    [output_jobs]
        [output_jobs.datahub]
        name = "datahub"
        schedule-spec = "@every 15s"
        lib-path = "/lib/outputlib/outputlib_datahub.so"
        lib-configuration = "/etc/alameda/federatorai-agent/output/alameda_datahub.toml"

  kubernetes.toml: |
    [datahub]
    address = "alameda-datahub.{{.NameSpace}}:50050"
    ignore_empty_cluster = true

    [route]
    container_name = "router"

    [datasource.kubernetes]
    datatype = "kubernetes"

    [[datasource.kubernetes.measurement]]
    name = "haproxy_server_http_average_response_latency_milliseconds"
    tags = ["exported_service", "exported_namespace"]
    [datasource.kubernetes.measurement.element.value]
    type = "float"

  alameda_app.toml: |
    [global]
    interval = 15
    timerange = 60
    granularity = 15

    [datasource.prometheus]
    datatype = "prometheus"
    url = "{{.Prometheus.Address}}"
    bearer_token_file = "{{.Prometheus.BearerTokenFile}}"
    insecure_skip_verify = {{.Prometheus.TLS.InsecureSkipVerify}}

    {{if .Nginx.Enabled}}
    [[datasource.prometheus.measurement]]
    name = "nginx_http_response_total"
    expr = "sum(delta(haproxy_server_http_responses_total[1m])) by (exported_service, exported_namespace)"
    tags = ["exported_service", "exported_namespace"]
    [datasource.prometheus.measurement.element.value]
    type = "float"
    {{end}}

  alameda_kafka.toml: |
    [global]
    interval = 60
    timerange = 180
    granularity = 60

    [datasource.prometheus]
    datatype = "prometheus"
    url = "{{.Prometheus.Address}}"
    bearer_token_file = "{{.Prometheus.BearerTokenFile}}"
    insecure_skip_verify = {{.Prometheus.TLS.InsecureSkipVerify}}

    {{if .Kafka.Enabled}}
    [[datasource.prometheus.measurement]]
    name = "kafka_topic_partition_current_offset"
    expr = "sum(delta(kafka_topic_partition_current_offset[3m])/3) by (consumergroup,topic,namespace)"
    tags = ["consumergroup","topic","namespace"]
    [datasource.prometheus.measurement.element.value]
    type = "float"

    [[datasource.prometheus.measurement]]
    name = "kafka_consumer_group_current_offset"
    expr = "sum(delta(kafka_consumergroup_current_offset[3m])/3) by (consumergroup,topic,namespace)"
    tags = ["consumergroup","topic","namespace"]
    [datasource.prometheus.measurement.element.value]
    type = "float"

    [[datasource.prometheus.measurement]]
    name = "kafka_consumer_group_lag"
    expr = "sum(kafka_consumergroup_lag) by (namespace, consumergroup, topic)"
    tags = ["consumergroup","topic","namespace"]
    [datasource.prometheus.measurement.element.value]
    type = "float"
    {{end}}

  alameda_node.toml: |
    [global]
    interval = 60
    timerange = 60
    granularity = 60

    [datasource.prometheus]
    datatype = "prometheus"
    url = "{{.Prometheus.Address}}"
    bearer_token_file = "{{.Prometheus.BearerTokenFile}}"
    insecure_skip_verify = {{.Prometheus.TLS.InsecureSkipVerify}}
    {{if .ClusterAutoScaler.EnableExecution}}
    [[datasource.prometheus.measurement]]
    name = "node_cpu"
    expr = "1000 * node:node_num_cpu:sum * node:node_cpu_utilisation:avg1m"
    tags = ["node"]
    [datasource.prometheus.measurement.element.value]
    type = "float"

    [[datasource.prometheus.measurement]]
    name = "node_memory"
    expr = "node:node_memory_utilisation_2: * node:node_memory_bytes_total:sum"
    tags = ["node"]
    [datasource.prometheus.measurement.element.value]
    type = "float"
    {{end}}

  alameda_datahub.toml: |
    [datahub]
    address = "alameda-datahub.{{.NameSpace}}:50050"

    [datahub."retry-interval"]
    default = 3 # second
